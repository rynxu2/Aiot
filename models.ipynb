{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 50  # 1 second of data at 50Hz\n",
    "N_FEATURES = 6  # AccelX,Y,Z and GyroX,Y,Z\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 3\n",
    "NUM_HEADS = 4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = torch.mean(x, dim=1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseek\n",
    "class TransformerModelDeepseek(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_size=64, num_layers=3, \n",
    "                 num_heads=4, num_classes=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.preprocess = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_size),\n",
    "            nn.Conv1d(input_size, hidden_size//2, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.position_embed = nn.Parameter(torch.randn(1, hidden_size//2, 1))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size//2,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size//2),\n",
    "            nn.Linear(hidden_size//2, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.preprocess(x)\n",
    "        x = x + self.position_embed\n",
    "        x = x.permute(0, 2, 1)  # (batch, seq_len, hidden_dim//2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch, hidden_dim//2, seq_len)\n",
    "        x = self.adaptive_pool(x).squeeze(-1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class TransformerModelChatGPT(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_size=128, num_layers=3, num_heads=4, num_classes=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.pos_encoding = PositionalEncoding(hidden_size)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = torch.mean(x, dim=1)  # Global average pooling\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grok\n",
    "class TransformerModelGrok(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 2,  # Giảm chi phí tính toán\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pooling = lambda x: torch.mean(x, dim=1)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grok optimal\n",
    "class PositionalEncodingGrok(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "class TransformerModelGrokOptimal(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, num_classes, dropout=0.1, max_len=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_size, hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.pos_encoding = PositionalEncodingGrok(hidden_size, max_len=max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pooling = lambda x: torch.cat([torch.mean(x, dim=1), torch.max(x, dim=1)[0]], dim=1)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = self.input_projection(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cursor optimal\n",
    "class TransformerModelCursorOptimal(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, num_classes, dropout=0.1, max_len=100):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(input_size, hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.pos_encoding = PositionalEncodingGrok(hidden_size, max_len)\n",
    "        self.dropout = nn.Dropout(dropout + 0.1)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout + 0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pooling = lambda x: torch.cat([\n",
    "            torch.mean(x, dim=1),\n",
    "            torch.max(x, dim=1)[0]\n",
    "        ], dim=1)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude optimal\n",
    "class TransformerModelClaudeOptimal(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_size=128, num_layers=3, num_heads=4, \n",
    "                 num_classes=2, seq_length=100, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.accel_projection = nn.Linear(3, hidden_size // 2)\n",
    "        self.gyro_projection = nn.Linear(3, hidden_size // 2)\n",
    "        self.freq_extractor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, seq_length, hidden_size))\n",
    "        nn.init.xavier_uniform_(self.pos_encoder)\n",
    "        self.feature_weights = nn.Parameter(torch.ones(2))\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.pooling_mean = nn.AdaptiveAvgPool1d(1)\n",
    "        self.pooling_max = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.act = nn.GELU()\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        if self.input_size >= 6:  # Đảm bảo có đủ kênh cho accel và gyro\n",
    "            accel_data = x[:, :, :3]  # 3 trục accelerometer\n",
    "            gyro_data = x[:, :, 3:6]  # 3 trục gyroscope\n",
    "            accel_features = self.accel_projection(accel_data)\n",
    "            gyro_features = self.gyro_projection(gyro_data)\n",
    "            x = torch.cat([accel_features, gyro_features], dim=2)\n",
    "        else:\n",
    "            x = nn.Linear(self.input_size, self.hidden_size)(x)\n",
    "        \n",
    "        x_time = x  # Đặc trưng thời gian là đầu vào ban đầu\n",
    "        x_freq = self.freq_extractor(x)  # Đặc trưng tần số\n",
    "        weights = F.softmax(self.feature_weights, dim=0)\n",
    "        x = weights[0] * x_time + weights[1] * x_freq\n",
    "        x = x + self.pos_encoder[:, :seq_length, :]\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        if mask is not None:\n",
    "            x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        else:\n",
    "            x = self.transformer(x)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + attn_output  # Residual connection\n",
    "        x = self.layer_norm2(x)\n",
    "        x_t = x.transpose(1, 2)  # [batch_size, hidden_size, seq_length]\n",
    "        mean_pool = self.pooling_mean(x_t).squeeze(-1)  # [batch_size, hidden_size]\n",
    "        max_pool = self.pooling_max(x_t).squeeze(-1)    # [batch_size, hidden_size]\n",
    "        x = torch.cat([mean_pool, max_pool], dim=1)  # [batch_size, hidden_size*2]\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseek optimal\n",
    "class TransformerModelDeepseekOptimal(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_size=128, num_layers=3, num_heads=4, num_classes=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size)\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=nn.GELU()  # Tăng tốc hội tụ\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # Tập trung vào đặc trưng quan trọng\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.transformer(x)  # (batch_size, seq_len, hidden_size)\n",
    "        x = x.permute(0, 2, 1)   # (batch_size, hidden_size, seq_len)\n",
    "        x = self.pooling(x).squeeze(-1)  # (batch_size, hidden_size)\n",
    "        x = self.fc(x)  # (batch_size, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(0, len(data) - sequence_length, sequence_length//2):  # 50% overlap\n",
    "        seq = data[i:i + sequence_length]\n",
    "        if len(seq) == sequence_length:\n",
    "            sequences.append(seq)\n",
    "            labels.append(seq['ActivityLabel'].mode()[0])  # Most common label in sequence\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Load data\n",
    "    df = pd.read_csv('data\\\\combined\\\\merged_users_corrected.csv')\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    features = ['AccelX', 'AccelY', 'AccelZ', 'GyroX', 'GyroY', 'GyroZ']\n",
    "    X = df[features]\n",
    "    y = df['ActivityLabel']\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=features)\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_scaled['ActivityLabel'] = y_encoded\n",
    "    sequences, labels = prepare_sequences(X_scaled, SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Remove ActivityLabel column from sequences\n",
    "    sequences = sequences[:, :, :-1].astype(np.float32)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        sequences, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ActivityDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "    test_dataset = ActivityDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = TransformerModelDeepseekOptimal(\n",
    "        input_size=N_FEATURES,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Training loop\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        accuracy = 100. * correct / total\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "    }, 'results\\\\TransformerModelDeepseekOptimal.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
